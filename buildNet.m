function net = buildNet(net,nChan,nPlates)
%Modify pre-trained texture description CNN for fine-tuning

import dagnn.*

net.meta.outputSize = [1 1];

net.removeLayer('top5err');
net.removeLayer('top1err');
net.removeLayer('loss');
net.removeLayer('conv_blend');

%modify first layer to adapt multiple channels
net.layers(1).block.size(3) = nChan;
net.params(1).value = repmat(net.params(1).value,1,1,nChan,1);

%Loss
name = 'loss';
block = HistogramLoss() ;
block.histDim = 100;
inputs = {'code','labels'};
outputs = {'objective'};

params = struct(...
  'name', {}, ...
  'value', {}, ...
  'learningRate', [], ...
  'weightDecay', []) ;

net.addLayer(...
  name, ...
  block, ...
  inputs, ...
  outputs, ...
  {params.name}) ;

name = 'errorHist';
block = HistogramLoss() ;
block.histDim = 100;
inputs = {'code','labels'};
outputs = {'errorHist'};

net.addLayer(...
  name, ...
  block, ...
  inputs, ...
  outputs) ;

if nPlates > 1
  
  %Batch effect control with adversarial training
  name = 's1_batchloss';
  block = Switch();
  inputs = {'code'};
  outputs = {'code_x'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
  
  lp = [1 1 1024*3 nPlates 1 0]; %Params of the block
  name = 'fc1';
  block = Conv() ;
  block.size = [lp(1) lp(2) lp(3) lp(4)];
  block.hasBias = true;
  block.opts{1} = 'cuDNN';
  block.pad = lp(6);
  block.stride = lp(5);
  
  inputs = {'code_x'};
  outputs = {'prediction'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  sc = sqrt(2/(lp(1)*lp(2)*lp(4))) ;
  params(1).name = sprintf('%sf',name) ;
  params(1).value = sc*randn(lp(1), lp(2), lp(3), lp(4), 'single') ;
  params(1).learningRate = 1 ;
  params(1).weightDecay = 1 ;
  
  params(2).name = sprintf('%sb',name) ;
  params(2).value = zeros(lp(4), 1, 'single') ;
  params(2).learningRate = 2 ;
  params(2).weightDecay = 0 ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
  
  findex = net.getParamIndex(params(1).name) ;
  net.params(findex).value = params(1).value ;
  net.params(findex).learningRate = params(1).learningRate ;
  net.params(findex).weightDecay = params(1).weightDecay ;
  
  bindex = net.getParamIndex(params(2).name) ;
  net.params(bindex).value = params(2).value ;
  net.params(bindex).learningRate = params(2).learningRate ;
  net.params(bindex).weightDecay = params(2).weightDecay ;
  
  %Loss
  name = 's2_batchloss';
  block = Switch();
  inputs = {'prediction'};
  outputs = {'prediction_x'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
  
  net.addLayer('batchloss', dagnn.Loss('loss', 'softmaxlog'), ...
    {'prediction_x','plate_labels'}, 'objective1') ;
  net.addLayer('top1err', dagnn.Loss('loss', 'classerror'), ...
    {'prediction_x','plate_labels'}, 'top1err') ;
  net.addLayer('top5err', dagnn.Loss('loss', 'topkerror', ...
    'opts', {'topK',5}), ...
    {'prediction_x','plate_labels'}, 'top5err') ;
  
  name = 'softmax';
  block = SoftMax();
  inputs = {'prediction'};
  outputs = {'sm_prediction'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
  
  name = 's1_klloss';
  block = Switch();
  inputs = {'sm_prediction'};
  outputs = {'sm_prediction_x'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
  
  name = 'KLloss';
  block = KLDivergence() ;
  block.inIsLog = false;
  inputs = {'sm_prediction_x','target_dist'};
  outputs = {'objective2'};
  
  params = struct(...
    'name', {}, ...
    'value', {}, ...
    'learningRate', [], ...
    'weightDecay', []) ;
  
  net.addLayer(...
    name, ...
    block, ...
    inputs, ...
    outputs, ...
    {params.name}) ;
end